diff -crwN linux-2.2.16/Documentation/Configure.help linux-2.2.16-papi/Documentation/Configure.help
*** linux-2.2.16/Documentation/Configure.help	Wed Jul 12 12:55:53 2000
--- linux-2.2.16-papi/Documentation/Configure.help	Thu Nov 16 15:01:46 2000
***************
*** 1689,1694 ****
--- 1689,1703 ----
  
    If you don't know what to do, choose "386".
  
+ CPU Performance Counter Support
+ CONFIG_PERF
+   This provides support (via a syscall) for hardware performance counters
+   present in the Pentium (untested), Pentium Pro and Pentium II. This patch
+   is a modification to that originally written by Erik Hendriks for the
+   Beowulf project at http://www.beowulf.org/software/software.html. This patch
+   is intended to provide some bug fixes and support for PAPI. See
+   http://icl.cs.utk.edu/projects/papi for further details. (mucci@cs.utk.edu)
+ 
  VGA text console
  CONFIG_VGA_CONSOLE
    Saying Y here will allow you to use Linux in text mode through a
diff -crwN linux-2.2.16/arch/i386/config.in linux-2.2.16-papi/arch/i386/config.in
*** linux-2.2.16/arch/i386/config.in	Wed Jul 12 12:55:50 2000
--- linux-2.2.16-papi/arch/i386/config.in	Thu Nov 16 15:01:46 2000
***************
*** 40,45 ****
--- 40,50 ----
  bool 'Math emulation' CONFIG_MATH_EMULATION
  bool 'MTRR (Memory Type Range Register) support' CONFIG_MTRR
  bool 'Symmetric multi-processing support' CONFIG_SMP
+ 
+ if [ "$CONFIG_M686" = "y" ]; then
+   bool 'CPU Performance Counter Support' CONFIG_PERF
+ fi
+ 
  endmenu
  
  mainmenu_option next_comment
diff -crwN linux-2.2.16/arch/i386/kernel/#entry.S.rej# linux-2.2.16-papi/arch/i386/kernel/#entry.S.rej#
*** linux-2.2.16/arch/i386/kernel/#entry.S.rej#	Wed Dec 31 19:00:00 1969
--- linux-2.2.16-papi/arch/i386/kernel/#entry.S.rej#	Thu Nov 16 15:03:44 2000
***************
*** 0 ****
--- 1,43 ----
+ ***************
+ *** 564,569 ****
+   	.long SYMBOL_NAME(sys_ni_syscall)		/* streams1 */
+   	.long SYMBOL_NAME(sys_ni_syscall)		/* streams2 */
+   	.long SYMBOL_NAME(sys_vfork)            /* 190 */
+   
+   	/*
+   	 * NOTE!! This doesn't have to be exact - we just have
+ --- 565,576 ----
+   	.long SYMBOL_NAME(sys_ni_syscall)		/* streams1 */
+   	.long SYMBOL_NAME(sys_ni_syscall)		/* streams2 */
+   	.long SYMBOL_NAME(sys_vfork)            /* 190 */
+ + #ifdef CONFIG_PERF
+ + 	.rept NR_syscalls-190-5
+ + 		.long SYMBOL_NAME(sys_ni_syscall)
+ + 	.endr
+ + 	.long SYMBOL_NAME(sys_perf)		/* 252 */
+ + #endif
+   
+   	/*
+   	 * NOTE!! This doesn't have to be exact - we just have
+ ***************
+ *** 571,576 ****
+   	 * entries. Don't panic if you notice that this hasn't
+   	 * been shrunk every time we add a new system call.
+   	 */
+   	.rept NR_syscalls-190
+   		.long SYMBOL_NAME(sys_ni_syscall)
+   	.endr
+ --- 578,589 ----
+   	 * entries. Don't panic if you notice that this hasn't
+   	 * been shrunk every time we add a new system call.
+   	 */
+ + #ifdef CONFIG_PERF
+ + 	.rept NR_syscalls-252
+ + 		.long SYMBOL_NAME(sys_ni_syscall)
+ + 	.endr
+ + #else
+   	.rept NR_syscalls-190
+   		.long SYMBOL_NAME(sys_ni_syscall)
+   	.endr
+ + #endif
+ 
diff -crwN linux-2.2.16/arch/i386/kernel/Makefile linux-2.2.16-papi/arch/i386/kernel/Makefile
*** linux-2.2.16/arch/i386/kernel/Makefile	Wed Jan 20 13:18:53 1999
--- linux-2.2.16-papi/arch/i386/kernel/Makefile	Thu Nov 16 15:01:46 2000
***************
*** 50,55 ****
--- 50,59 ----
  O_OBJS += visws_apic.o
  endif
  
+ ifdef CONFIG_PERF
+ O_OBJS += perf.o
+ endif
+ 
  head.o: head.S $(TOPDIR)/include/linux/tasks.h
  	$(CC) -D__ASSEMBLY__ $(AFLAGS) -traditional -c $*.S -o $*.o
  
diff -crwN linux-2.2.16/arch/i386/kernel/entry.S linux-2.2.16-papi/arch/i386/kernel/entry.S
*** linux-2.2.16/arch/i386/kernel/entry.S	Wed Jul 12 12:55:54 2000
--- linux-2.2.16-papi/arch/i386/kernel/entry.S	Thu Nov 16 15:09:30 2000
***************
*** 40,45 ****
--- 40,46 ----
   * "current" is in register %ebx during any slow entries.
   */
  
+ #include <linux/config.h>
  #include <linux/sys.h>
  #include <linux/linkage.h>
  #include <asm/segment.h>
***************
*** 571,577 ****
  	.long SYMBOL_NAME(sys_stat64)		/* 195 */
  	.long SYMBOL_NAME(sys_lstat64)
  	.long SYMBOL_NAME(sys_fstat64)
! 
  
  	/*
  	 * NOTE!! This doesn't have to be exact - we just have
--- 572,583 ----
  	.long SYMBOL_NAME(sys_stat64)		/* 195 */
  	.long SYMBOL_NAME(sys_lstat64)
  	.long SYMBOL_NAME(sys_fstat64)
! #ifdef CONFIG_PERF
! 	.rept NR_syscalls-197-5
! 		.long SYMBOL_NAME(sys_ni_syscall)
! 	.endr
! 	.long SYMBOL_NAME(sys_perf)             /* 252 */
! #endif
  	
  	/*
  	 * NOTE!! This doesn't have to be exact - we just have
***************
*** 579,584 ****
--- 585,596 ----
  	 * entries. Don't panic if you notice that this hasn't
  	 * been shrunk every time we add a new system call.
  	 */
+ #ifdef CONFIG_PERF
+ 	.rept NR_syscalls-252
+ 		.long SYMBOL_NAME(sys_ni_syscall)
+ 	.endr
+ #else
  	.rept NR_syscalls-197
  		.long SYMBOL_NAME(sys_ni_syscall)
  	.endr
+ #endif
diff -crwN linux-2.2.16/arch/i386/kernel/entry.S~ linux-2.2.16-papi/arch/i386/kernel/entry.S~
*** linux-2.2.16/arch/i386/kernel/entry.S~	Wed Dec 31 19:00:00 1969
--- linux-2.2.16-papi/arch/i386/kernel/entry.S~	Thu Nov 16 15:01:46 2000
***************
*** 0 ****
--- 1,585 ----
+ /*
+  *  linux/arch/i386/entry.S
+  *
+  *  Copyright (C) 1991, 1992  Linus Torvalds
+  */
+ 
+ /*
+  * entry.S contains the system-call and fault low-level handling routines.
+  * This also contains the timer-interrupt handler, as well as all interrupts
+  * and faults that can result in a task-switch.
+  *
+  * NOTE: This code handles signal-recognition, which happens every time
+  * after a timer-interrupt and after each system call.
+  *
+  * I changed all the .align's to 4 (16 byte alignment), as that's faster
+  * on a 486.
+  *
+  * Stack layout in 'ret_from_system_call':
+  * 	ptrace needs to have all regs on the stack.
+  *	if the order here is changed, it needs to be
+  *	updated in fork.c:copy_process, signal.c:do_signal,
+  *	ptrace.c and ptrace.h
+  *
+  *	 0(%esp) - %ebx
+  *	 4(%esp) - %ecx
+  *	 8(%esp) - %edx
+  *       C(%esp) - %esi
+  *	10(%esp) - %edi
+  *	14(%esp) - %ebp
+  *	18(%esp) - %eax
+  *	1C(%esp) - %ds
+  *	20(%esp) - %es
+  *	24(%esp) - orig_eax
+  *	28(%esp) - %eip
+  *	2C(%esp) - %cs
+  *	30(%esp) - %eflags
+  *	34(%esp) - %oldesp
+  *	38(%esp) - %oldss
+  *
+  * "current" is in register %ebx during any slow entries.
+  */
+ 
+ #include <linux/config.h>
+ #include <linux/sys.h>
+ #include <linux/linkage.h>
+ #include <asm/segment.h>
+ #define ASSEMBLY
+ #include <asm/smp.h>
+ 
+ EBX		= 0x00
+ ECX		= 0x04
+ EDX		= 0x08
+ ESI		= 0x0C
+ EDI		= 0x10
+ EBP		= 0x14
+ EAX		= 0x18
+ DS		= 0x1C
+ ES		= 0x20
+ ORIG_EAX	= 0x24
+ EIP		= 0x28
+ CS		= 0x2C
+ EFLAGS		= 0x30
+ OLDESP		= 0x34
+ OLDSS		= 0x38
+ 
+ CF_MASK		= 0x00000001
+ IF_MASK		= 0x00000200
+ NT_MASK		= 0x00004000
+ VM_MASK		= 0x00020000
+ 
+ /*
+  * these are offsets into the task-struct.
+  */
+ state		=  0
+ flags		=  4
+ sigpending	=  8
+ addr_limit	= 12
+ exec_domain	= 16
+ need_resched	= 20
+ 
+ ENOSYS = 38
+ 
+ 
+ #define SAVE_ALL \
+ 	cld; \
+ 	pushl %es; \
+ 	pushl %ds; \
+ 	pushl %eax; \
+ 	pushl %ebp; \
+ 	pushl %edi; \
+ 	pushl %esi; \
+ 	pushl %edx; \
+ 	pushl %ecx; \
+ 	pushl %ebx; \
+ 	movl $(__KERNEL_DS),%edx; \
+ 	movl %dx,%ds; \
+ 	movl %dx,%es;
+ 
+ #define RESTORE_ALL	\
+ 	popl %ebx;	\
+ 	popl %ecx;	\
+ 	popl %edx;	\
+ 	popl %esi;	\
+ 	popl %edi;	\
+ 	popl %ebp;	\
+ 	popl %eax;	\
+ 1:	popl %ds;	\
+ 2:	popl %es;	\
+ 	addl $4,%esp;	\
+ 3:	iret;		\
+ .section .fixup,"ax";	\
+ 4:	movl $0,(%esp);	\
+ 	jmp 1b;		\
+ 5:	movl $0,(%esp);	\
+ 	jmp 2b;		\
+ 6:	pushl %ss;	\
+ 	popl %ds;	\
+ 	pushl %ss;	\
+ 	popl %es;	\
+ 	pushl $11;	\
+ 	call do_exit;	\
+ .previous;		\
+ .section __ex_table,"a";\
+ 	.align 4;	\
+ 	.long 1b,4b;	\
+ 	.long 2b,5b;	\
+ 	.long 3b,6b;	\
+ .previous
+ 
+ #define GET_CURRENT(reg) \
+ 	movl %esp, reg; \
+ 	andl $-8192, reg;
+ 
+ ENTRY(lcall7)
+ 	pushfl			# We get a different stack layout with call gates,
+ 	pushl %eax		# which has to be cleaned up later..
+ 	SAVE_ALL
+ 	movl EIP(%esp),%eax	# due to call gates, this is eflags, not eip..
+ 	movl CS(%esp),%edx	# this is eip..
+ 	movl EFLAGS(%esp),%ecx	# and this is cs..
+ 	movl %eax,EFLAGS(%esp)	#
+ 	movl %edx,EIP(%esp)	# Now we move them to their "normal" places
+ 	movl %ecx,CS(%esp)	#
+ 	movl %esp,%ebx
+ 	pushl %ebx
+ 	andl $-8192,%ebx	# GET_CURRENT
+ 	movl exec_domain(%ebx),%edx	# Get the execution domain
+ 	movl 4(%edx),%edx	# Get the lcall7 handler for the domain
+ 	call *%edx
+ 	popl %eax
+ 	jmp ret_from_sys_call
+ 
+ 
+ 	ALIGN
+ 	.globl	ret_from_fork
+ ret_from_fork:
+ #ifdef __SMP__
+ 	pushl %ebx
+ 	call SYMBOL_NAME(schedule_tail)
+ 	addl $4, %esp
+ #endif /* __SMP__ */
+ 	GET_CURRENT(%ebx)
+ 	jmp	ret_from_sys_call
+ 
+ /*
+  * Return to user mode is not as complex as all this looks,
+  * but we want the default path for a system call return to
+  * go as quickly as possible which is why some of this is
+  * less clear than it otherwise should be.
+  */
+ 
+ ENTRY(system_call)
+ 	pushl %eax			# save orig_eax
+ 	SAVE_ALL
+ 	GET_CURRENT(%ebx)
+ 	cmpl $(NR_syscalls),%eax
+ 	jae badsys
+ 	testb $0x20,flags(%ebx)		# PF_TRACESYS
+ 	jne tracesys
+ 	call *SYMBOL_NAME(sys_call_table)(,%eax,4)
+ 	movl %eax,EAX(%esp)		# save the return value
+ 	ALIGN
+ 	.globl ret_from_sys_call
+ 	.globl ret_from_intr
+ ret_from_sys_call:
+ 	movl SYMBOL_NAME(bh_mask),%eax
+ 	andl SYMBOL_NAME(bh_active),%eax
+ 	jne handle_bottom_half
+ ret_with_reschedule:
+ 	cmpl $0,need_resched(%ebx)
+ 	jne reschedule
+ 	cmpl $0,sigpending(%ebx)
+ 	jne signal_return
+ restore_all:
+ 	RESTORE_ALL
+ 
+ 	ALIGN
+ signal_return:
+ 	sti				# we can get here from an interrupt handler
+ 	testl $(VM_MASK),EFLAGS(%esp)
+ 	movl %esp,%eax
+ 	jne v86_signal_return
+ 	xorl %edx,%edx
+ 	call SYMBOL_NAME(do_signal)
+ 	jmp restore_all
+ 
+ 	ALIGN
+ v86_signal_return:
+ 	call SYMBOL_NAME(save_v86_state)
+ 	movl %eax,%esp
+ 	xorl %edx,%edx
+ 	call SYMBOL_NAME(do_signal)
+ 	jmp restore_all
+ 
+ 	ALIGN
+ tracesys:
+ 	movl $-ENOSYS,EAX(%esp)
+ 	call SYMBOL_NAME(syscall_trace)
+ 	movl ORIG_EAX(%esp),%eax
+         cmpl $(NR_syscalls),%eax 
+         jae 1f 
+ 	call *SYMBOL_NAME(sys_call_table)(,%eax,4)
+ 	movl %eax,EAX(%esp)		# save the return value
+ 1:	call SYMBOL_NAME(syscall_trace)
+ 	jmp ret_from_sys_call
+ badsys:
+ 	movl $-ENOSYS,EAX(%esp)
+ 	jmp ret_from_sys_call
+ 
+ 	ALIGN
+ ret_from_exception:
+ 	movl SYMBOL_NAME(bh_mask),%eax
+ 	andl SYMBOL_NAME(bh_active),%eax
+ 	jne handle_bottom_half
+ 	ALIGN
+ ret_from_intr:
+ 	GET_CURRENT(%ebx)
+ 	movl EFLAGS(%esp),%eax		# mix EFLAGS and CS
+ 	movb CS(%esp),%al
+ 	testl $(VM_MASK | 3),%eax	# return to VM86 mode or non-supervisor?
+ 	jne ret_with_reschedule
+ 	jmp restore_all
+ 
+ 	ALIGN
+ handle_bottom_half:
+ 	call SYMBOL_NAME(do_bottom_half)
+ 	jmp ret_from_intr
+ 
+ 	ALIGN
+ reschedule:
+ 	call SYMBOL_NAME(schedule)    # test
+ 	jmp ret_from_sys_call
+ 
+ ENTRY(divide_error)
+ 	pushl $0		# no error code
+ 	pushl $ SYMBOL_NAME(do_divide_error)
+ 	ALIGN
+ error_code:
+ 	pushl %ds
+ 	pushl %eax
+ 	xorl %eax,%eax
+ 	pushl %ebp
+ 	pushl %edi
+ 	pushl %esi
+ 	pushl %edx
+ 	decl %eax			# eax = -1
+ 	pushl %ecx
+ 	pushl %ebx
+ 	cld
+ 	movl %es,%cx
+ 	xchgl %eax, ORIG_EAX(%esp)	# orig_eax (get the error code. )
+ 	movl %esp,%edx
+ 	xchgl %ecx, ES(%esp)		# get the address and save es.
+ 	pushl %eax			# push the error code
+ 	pushl %edx
+ 	movl $(__KERNEL_DS),%edx
+ 	movl %dx,%ds
+ 	movl %dx,%es
+ 	GET_CURRENT(%ebx)
+ 	call *%ecx
+ 	addl $8,%esp
+ 	jmp ret_from_exception
+ 
+ ENTRY(coprocessor_error)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_coprocessor_error)
+ 	jmp error_code
+ 
+ ENTRY(device_not_available)
+ 	pushl $-1		# mark this as an int
+ 	SAVE_ALL
+ 	GET_CURRENT(%ebx)
+ 	pushl $ret_from_exception
+ 	movl %cr0,%eax
+ 	testl $0x4,%eax			# EM (math emulation bit)
+ 	je SYMBOL_NAME(math_state_restore)
+ 	pushl $0		# temporary storage for ORIG_EIP
+ 	call  SYMBOL_NAME(math_emulate)
+ 	addl $4,%esp
+ 	ret
+ 
+ ENTRY(debug)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_debug)
+ 	jmp error_code
+ 
+ ENTRY(nmi)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_nmi)
+ 	jmp error_code
+ 
+ ENTRY(int3)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_int3)
+ 	jmp error_code
+ 
+ ENTRY(overflow)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_overflow)
+ 	jmp error_code
+ 
+ ENTRY(bounds)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_bounds)
+ 	jmp error_code
+ 
+ ENTRY(invalid_op)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_invalid_op)
+ 	jmp error_code
+ 
+ ENTRY(coprocessor_segment_overrun)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_coprocessor_segment_overrun)
+ 	jmp error_code
+ 
+ ENTRY(reserved)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_reserved)
+ 	jmp error_code
+ 
+ ENTRY(double_fault)
+ 	pushl $ SYMBOL_NAME(do_double_fault)
+ 	jmp error_code
+ 
+ ENTRY(invalid_TSS)
+ 	pushl $ SYMBOL_NAME(do_invalid_TSS)
+ 	jmp error_code
+ 
+ ENTRY(segment_not_present)
+ 	pushl $ SYMBOL_NAME(do_segment_not_present)
+ 	jmp error_code
+ 
+ ENTRY(stack_segment)
+ 	pushl $ SYMBOL_NAME(do_stack_segment)
+ 	jmp error_code
+ 
+ ENTRY(general_protection)
+ 	pushl $ SYMBOL_NAME(do_general_protection)
+ 	jmp error_code
+ 
+ ENTRY(alignment_check)
+ 	pushl $ SYMBOL_NAME(do_alignment_check)
+ 	jmp error_code
+ 
+ ENTRY(page_fault)
+ 	pushl $ SYMBOL_NAME(do_page_fault)
+ 	jmp error_code
+ 
+ ENTRY(spurious_interrupt_bug)
+ 	pushl $0
+ 	pushl $ SYMBOL_NAME(do_spurious_interrupt_bug)
+ 	jmp error_code
+ 
+ .data
+ ENTRY(sys_call_table)
+ 	.long SYMBOL_NAME(sys_ni_syscall)	/* 0  -  old "setup()" system call*/
+ 	.long SYMBOL_NAME(sys_exit)
+ 	.long SYMBOL_NAME(sys_fork)
+ 	.long SYMBOL_NAME(sys_read)
+ 	.long SYMBOL_NAME(sys_write)
+ 	.long SYMBOL_NAME(sys_open)		/* 5 */
+ 	.long SYMBOL_NAME(sys_close)
+ 	.long SYMBOL_NAME(sys_waitpid)
+ 	.long SYMBOL_NAME(sys_creat)
+ 	.long SYMBOL_NAME(sys_link)
+ 	.long SYMBOL_NAME(sys_unlink)		/* 10 */
+ 	.long SYMBOL_NAME(sys_execve)
+ 	.long SYMBOL_NAME(sys_chdir)
+ 	.long SYMBOL_NAME(sys_time)
+ 	.long SYMBOL_NAME(sys_mknod)
+ 	.long SYMBOL_NAME(sys_chmod)		/* 15 */
+ 	.long SYMBOL_NAME(sys_lchown)
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old break syscall holder */
+ 	.long SYMBOL_NAME(sys_stat)
+ 	.long SYMBOL_NAME(sys_lseek)
+ 	.long SYMBOL_NAME(sys_getpid)		/* 20 */
+ 	.long SYMBOL_NAME(sys_mount)
+ 	.long SYMBOL_NAME(sys_oldumount)
+ 	.long SYMBOL_NAME(sys_setuid)
+ 	.long SYMBOL_NAME(sys_getuid)
+ 	.long SYMBOL_NAME(sys_stime)		/* 25 */
+ 	.long SYMBOL_NAME(sys_ptrace)
+ 	.long SYMBOL_NAME(sys_alarm)
+ 	.long SYMBOL_NAME(sys_fstat)
+ 	.long SYMBOL_NAME(sys_pause)
+ 	.long SYMBOL_NAME(sys_utime)		/* 30 */
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old stty syscall holder */
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old gtty syscall holder */
+ 	.long SYMBOL_NAME(sys_access)
+ 	.long SYMBOL_NAME(sys_nice)
+ 	.long SYMBOL_NAME(sys_ni_syscall)	/* 35 */		/* old ftime syscall holder */
+ 	.long SYMBOL_NAME(sys_sync)
+ 	.long SYMBOL_NAME(sys_kill)
+ 	.long SYMBOL_NAME(sys_rename)
+ 	.long SYMBOL_NAME(sys_mkdir)
+ 	.long SYMBOL_NAME(sys_rmdir)		/* 40 */
+ 	.long SYMBOL_NAME(sys_dup)
+ 	.long SYMBOL_NAME(sys_pipe)
+ 	.long SYMBOL_NAME(sys_times)
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old prof syscall holder */
+ 	.long SYMBOL_NAME(sys_brk)		/* 45 */
+ 	.long SYMBOL_NAME(sys_setgid)
+ 	.long SYMBOL_NAME(sys_getgid)
+ 	.long SYMBOL_NAME(sys_signal)
+ 	.long SYMBOL_NAME(sys_geteuid)
+ 	.long SYMBOL_NAME(sys_getegid)		/* 50 */
+ 	.long SYMBOL_NAME(sys_acct)
+ 	.long SYMBOL_NAME(sys_umount)					/* recycled never used phys() */
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old lock syscall holder */
+ 	.long SYMBOL_NAME(sys_ioctl)
+ 	.long SYMBOL_NAME(sys_fcntl)		/* 55 */
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old mpx syscall holder */
+ 	.long SYMBOL_NAME(sys_setpgid)
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old ulimit syscall holder */
+ 	.long SYMBOL_NAME(sys_olduname)
+ 	.long SYMBOL_NAME(sys_umask)		/* 60 */
+ 	.long SYMBOL_NAME(sys_chroot)
+ 	.long SYMBOL_NAME(sys_ustat)
+ 	.long SYMBOL_NAME(sys_dup2)
+ 	.long SYMBOL_NAME(sys_getppid)
+ 	.long SYMBOL_NAME(sys_getpgrp)		/* 65 */
+ 	.long SYMBOL_NAME(sys_setsid)
+ 	.long SYMBOL_NAME(sys_sigaction)
+ 	.long SYMBOL_NAME(sys_sgetmask)
+ 	.long SYMBOL_NAME(sys_ssetmask)
+ 	.long SYMBOL_NAME(sys_setreuid)		/* 70 */
+ 	.long SYMBOL_NAME(sys_setregid)
+ 	.long SYMBOL_NAME(sys_sigsuspend)
+ 	.long SYMBOL_NAME(sys_sigpending)
+ 	.long SYMBOL_NAME(sys_sethostname)
+ 	.long SYMBOL_NAME(sys_setrlimit)	/* 75 */
+ 	.long SYMBOL_NAME(sys_getrlimit)
+ 	.long SYMBOL_NAME(sys_getrusage)
+ 	.long SYMBOL_NAME(sys_gettimeofday)
+ 	.long SYMBOL_NAME(sys_settimeofday)
+ 	.long SYMBOL_NAME(sys_getgroups)	/* 80 */
+ 	.long SYMBOL_NAME(sys_setgroups)
+ 	.long SYMBOL_NAME(old_select)
+ 	.long SYMBOL_NAME(sys_symlink)
+ 	.long SYMBOL_NAME(sys_lstat)
+ 	.long SYMBOL_NAME(sys_readlink)		/* 85 */
+ 	.long SYMBOL_NAME(sys_uselib)
+ 	.long SYMBOL_NAME(sys_swapon)
+ 	.long SYMBOL_NAME(sys_reboot)
+ 	.long SYMBOL_NAME(old_readdir)
+ 	.long SYMBOL_NAME(old_mmap)		/* 90 */
+ 	.long SYMBOL_NAME(sys_munmap)
+ 	.long SYMBOL_NAME(sys_truncate)
+ 	.long SYMBOL_NAME(sys_ftruncate)
+ 	.long SYMBOL_NAME(sys_fchmod)
+ 	.long SYMBOL_NAME(sys_fchown)		/* 95 */
+ 	.long SYMBOL_NAME(sys_getpriority)
+ 	.long SYMBOL_NAME(sys_setpriority)
+ 	.long SYMBOL_NAME(sys_ni_syscall)				/* old profil syscall holder */
+ 	.long SYMBOL_NAME(sys_statfs)
+ 	.long SYMBOL_NAME(sys_fstatfs)		/* 100 */
+ 	.long SYMBOL_NAME(sys_ioperm)
+ 	.long SYMBOL_NAME(sys_socketcall)
+ 	.long SYMBOL_NAME(sys_syslog)
+ 	.long SYMBOL_NAME(sys_setitimer)
+ 	.long SYMBOL_NAME(sys_getitimer)	/* 105 */
+ 	.long SYMBOL_NAME(sys_newstat)
+ 	.long SYMBOL_NAME(sys_newlstat)
+ 	.long SYMBOL_NAME(sys_newfstat)
+ 	.long SYMBOL_NAME(sys_uname)
+ 	.long SYMBOL_NAME(sys_iopl)		/* 110 */
+ 	.long SYMBOL_NAME(sys_vhangup)
+ 	.long SYMBOL_NAME(sys_idle)
+ 	.long SYMBOL_NAME(sys_vm86old)
+ 	.long SYMBOL_NAME(sys_wait4)
+ 	.long SYMBOL_NAME(sys_swapoff)		/* 115 */
+ 	.long SYMBOL_NAME(sys_sysinfo)
+ 	.long SYMBOL_NAME(sys_ipc)
+ 	.long SYMBOL_NAME(sys_fsync)
+ 	.long SYMBOL_NAME(sys_sigreturn)
+ 	.long SYMBOL_NAME(sys_clone)		/* 120 */
+ 	.long SYMBOL_NAME(sys_setdomainname)
+ 	.long SYMBOL_NAME(sys_newuname)
+ 	.long SYMBOL_NAME(sys_modify_ldt)
+ 	.long SYMBOL_NAME(sys_adjtimex)
+ 	.long SYMBOL_NAME(sys_mprotect)		/* 125 */
+ 	.long SYMBOL_NAME(sys_sigprocmask)
+ 	.long SYMBOL_NAME(sys_create_module)
+ 	.long SYMBOL_NAME(sys_init_module)
+ 	.long SYMBOL_NAME(sys_delete_module)
+ 	.long SYMBOL_NAME(sys_get_kernel_syms)	/* 130 */
+ 	.long SYMBOL_NAME(sys_quotactl)
+ 	.long SYMBOL_NAME(sys_getpgid)
+ 	.long SYMBOL_NAME(sys_fchdir)
+ 	.long SYMBOL_NAME(sys_bdflush)
+ 	.long SYMBOL_NAME(sys_sysfs)		/* 135 */
+ 	.long SYMBOL_NAME(sys_personality)
+ 	.long SYMBOL_NAME(sys_ni_syscall)	/* for afs_syscall */
+ 	.long SYMBOL_NAME(sys_setfsuid)
+ 	.long SYMBOL_NAME(sys_setfsgid)
+ 	.long SYMBOL_NAME(sys_llseek)		/* 140 */
+ 	.long SYMBOL_NAME(sys_getdents)
+ 	.long SYMBOL_NAME(sys_select)
+ 	.long SYMBOL_NAME(sys_flock)
+ 	.long SYMBOL_NAME(sys_msync)
+ 	.long SYMBOL_NAME(sys_readv)		/* 145 */
+ 	.long SYMBOL_NAME(sys_writev)
+ 	.long SYMBOL_NAME(sys_getsid)
+ 	.long SYMBOL_NAME(sys_fdatasync)
+ 	.long SYMBOL_NAME(sys_sysctl)
+ 	.long SYMBOL_NAME(sys_mlock)		/* 150 */
+ 	.long SYMBOL_NAME(sys_munlock)
+ 	.long SYMBOL_NAME(sys_mlockall)
+ 	.long SYMBOL_NAME(sys_munlockall)
+ 	.long SYMBOL_NAME(sys_sched_setparam)
+ 	.long SYMBOL_NAME(sys_sched_getparam)   /* 155 */
+ 	.long SYMBOL_NAME(sys_sched_setscheduler)
+ 	.long SYMBOL_NAME(sys_sched_getscheduler)
+ 	.long SYMBOL_NAME(sys_sched_yield)
+ 	.long SYMBOL_NAME(sys_sched_get_priority_max)
+ 	.long SYMBOL_NAME(sys_sched_get_priority_min)  /* 160 */
+ 	.long SYMBOL_NAME(sys_sched_rr_get_interval)
+ 	.long SYMBOL_NAME(sys_nanosleep)
+ 	.long SYMBOL_NAME(sys_mremap)
+ 	.long SYMBOL_NAME(sys_setresuid)
+ 	.long SYMBOL_NAME(sys_getresuid)	/* 165 */
+ 	.long SYMBOL_NAME(sys_vm86)
+ 	.long SYMBOL_NAME(sys_query_module)
+ 	.long SYMBOL_NAME(sys_poll)
+ 	.long SYMBOL_NAME(sys_nfsservctl)
+ 	.long SYMBOL_NAME(sys_setresgid)	/* 170 */
+ 	.long SYMBOL_NAME(sys_getresgid)
+ 	.long SYMBOL_NAME(sys_prctl)
+ 	.long SYMBOL_NAME(sys_rt_sigreturn)
+ 	.long SYMBOL_NAME(sys_rt_sigaction)
+ 	.long SYMBOL_NAME(sys_rt_sigprocmask)	/* 175 */
+ 	.long SYMBOL_NAME(sys_rt_sigpending)
+ 	.long SYMBOL_NAME(sys_rt_sigtimedwait)
+ 	.long SYMBOL_NAME(sys_rt_sigqueueinfo)
+ 	.long SYMBOL_NAME(sys_rt_sigsuspend)
+ 	.long SYMBOL_NAME(sys_pread)		/* 180 */
+ 	.long SYMBOL_NAME(sys_pwrite)
+ 	.long SYMBOL_NAME(sys_chown)
+ 	.long SYMBOL_NAME(sys_getcwd)
+ 	.long SYMBOL_NAME(sys_capget)
+ 	.long SYMBOL_NAME(sys_capset)           /* 185 */
+ 	.long SYMBOL_NAME(sys_sigaltstack)
+ 	.long SYMBOL_NAME(sys_sendfile)
+ 	.long SYMBOL_NAME(sys_ni_syscall)		/* streams1 */
+ 	.long SYMBOL_NAME(sys_ni_syscall)		/* streams2 */
+ 	.long SYMBOL_NAME(sys_vfork)            /* 190 */
+ 	.long SYMBOL_NAME(sys_ni_syscall)
+ 	.long SYMBOL_NAME(sys_mmap2)
+ 	.long SYMBOL_NAME(sys_truncate64)
+ 	.long SYMBOL_NAME(sys_ftruncate64)
+ 	.long SYMBOL_NAME(sys_stat64)		/* 195 */
+ 	.long SYMBOL_NAME(sys_lstat64)
+ 	.long SYMBOL_NAME(sys_fstat64)
+ 
+ 
+ 	/*
+ 	 * NOTE!! This doesn't have to be exact - we just have
+ 	 * to make sure we have _enough_ of the "sys_ni_syscall"
+ 	 * entries. Don't panic if you notice that this hasn't
+ 	 * been shrunk every time we add a new system call.
+ 	 */
+ 	.rept NR_syscalls-197
+ 		.long SYMBOL_NAME(sys_ni_syscall)
+ 	.endr
diff -crwN linux-2.2.16/arch/i386/kernel/irq.c linux-2.2.16-papi/arch/i386/kernel/irq.c
*** linux-2.2.16/arch/i386/kernel/irq.c	Wed Jun  7 17:26:42 2000
--- linux-2.2.16-papi/arch/i386/kernel/irq.c	Thu Nov 16 15:01:46 2000
***************
*** 324,329 ****
--- 324,332 ----
  BUILD_SMP_INTERRUPT(stop_cpu_interrupt)
  BUILD_SMP_INTERRUPT(call_function_interrupt)
  BUILD_SMP_INTERRUPT(spurious_interrupt)
+ #ifdef CONFIG_PERF
+ BUILD_SMP_INTERRUPT(perf_sys_update_interrupt)
+ #endif
  
  /*
   * every pentium local APIC has two 'local interrupts', with a
***************
*** 1131,1136 ****
--- 1134,1143 ----
  
  	/* IPI vector for APIC spurious interrupts */
  	set_intr_gate(SPURIOUS_APIC_VECTOR, spurious_interrupt);
+ #ifdef CONFIG_PERF
+ 	/* IPI for perf counter update */
+ 	set_intr_gate(PERF_SYS_UPDATE_VECTOR,perf_sys_update_interrupt);
+ #endif
  #endif	
  	request_region(0x20,0x20,"pic1");
  	request_region(0xa0,0x20,"pic2");
diff -crwN linux-2.2.16/arch/i386/kernel/irq.h linux-2.2.16-papi/arch/i386/kernel/irq.h
*** linux-2.2.16/arch/i386/kernel/irq.h	Wed Jul 12 13:24:57 2000
--- linux-2.2.16-papi/arch/i386/kernel/irq.h	Thu Nov 16 15:01:46 2000
***************
*** 73,78 ****
--- 73,80 ----
   */
  #define IRQ0_TRAP_VECTOR	0x51
  
+ #define PERF_SYS_UPDATE_VECTOR	0x52
+ 
  /*
   * This IRQ should never happen, but we print a message nevertheless.
   */
diff -crwN linux-2.2.16/arch/i386/kernel/perf.c linux-2.2.16-papi/arch/i386/kernel/perf.c
*** linux-2.2.16/arch/i386/kernel/perf.c	Wed Dec 31 19:00:00 1969
--- linux-2.2.16-papi/arch/i386/kernel/perf.c	Thu Nov 16 15:01:46 2000
***************
*** 0 ****
--- 1,387 ----
+ /* Modified by Philip J. Mucci. mucci@cs.utk.edu (Feb-July 1999)
+    for PAPI. http://icl.cs.utk.edu/projects/papi
+ 
+    Major Modifications:
+    
+    This kernel externsion now supports a third counter. Counter 2 is
+    a software only, per process cycle counter. 
+    It counts WALL clock cycles. Counting domain settings
+    have no effect on it. To turn it on, write a 1 to configuration register
+    2 in either a call to PERF_FASTCONFIG or PERF_SET_CONFIG.
+ 
+    Added calls:
+ 
+    PERF_SYS_RESET_COUNTERS: 
+    	- Resets the values of the counters without changing configuration
+ 	registers.
+    PERF_RESET_COUNTERS: 
+    	- Resets the values of the counters without changing configuration
+ 	registers.
+    PERF_FASTCONFIG:
+    	- Configuration register transfer. 1st arg is a pointer
+ 	to an array of 3 unsigned int's. We don't need to PERF_ENABLE
+ 	if you set the proper bits on counter 0. This saves 1000's of 
+ 	cycles.
+    PERF_FASTREAD:
+    	- Read the array of 3 unsigned long long's from the scheduler
+ 	structure into arg1. This saves 1000's of cycles.
+    PERF_FASTWRITE:
+    	- Write the array of 3 unsigned long long's to the scheduler
+ 	structure from arg1. This saves 1000's of cycles.
+    PERF_GET_OPT:
+    PERF_SET_OPT:
+    
+    Added options:
+ 
+    PERF_SUM_CPUS
+    	- To return cumulative counts when using the SYS interface.
+    PERF_DO_CHILDREN
+    	- Child processes/threads inherit the counting hardware setting
+ 	and lock the use of their interface. When the parent calls wait()
+ 	the values are automatically added to those in the parent. 
+ */
+ 
+ /*
+  * Pentium Pro hardware performance counter support
+  *
+  * Erik Hendriks <hendriks@cesdis.gsfc.nasa.gov>
+  * (Feb 12 1998)
+  */
+ #include <linux/kernel.h>
+ #include <linux/sched.h>
+ #include <linux/mm.h>
+ #include <linux/errno.h>
+ #include <linux/smp_lock.h>
+ #include <asm/uaccess.h>
+ #include <asm/perf.h>
+ 
+ int do_wait(pid_t pid, unsigned int *stat_addr, int options,
+ 	    struct rusage *ru, unsigned long long *perf_counters);
+ 
+ volatile int              perf_sys_flag=0;
+ static unsigned int       perf_sys_conf   [NR_CPUS][PERF_COUNTERS];
+ static unsigned long long perf_sys_counter[NR_CPUS][PERF_COUNTERS];
+ static unsigned long long perf_sys_shadow_tsc[NR_CPUS];
+ 
+ /*--------------------------------------------------------------------
+  *  System wide counter setup routines.
+  */
+ void perf_sys_remote_sync(void) {
+     int i, id;
+     unsigned long flags;
+     id = smp_processor_id();
+     save_flags(flags);
+     cli();
+     for (i=0; i < PERF_COUNTERS-1; i++) {
+ 	/* Read the value of the performance counter, update the
+ 	 * config, reset the counter to zero. */
+ 	perf_sys_counter[id][i] += (perf_rdmsr(MSR_PERFCTR0+i) & ((1ULL<<40)-1));
+ 	perf_wrmsr(MSR_EVNTSEL0+i, perf_sys_conf[id][i]);
+ 	perf_wrmsr(MSR_PERFCTR0+i, 0);
+     }
+     if (perf_sys_conf[id][2])
+ 	perf_sys_counter[id][2] += perf_get_cycles() - current->perf.shadow_tsc;
+     current->perf.shadow_tsc = perf_get_cycles();
+     restore_flags(flags);
+ }
+ 
+ static
+ void sys_sync_counters(void) {
+ #ifdef __SMP__
+     smp_perf_sys_update();	/* Defined in arch/i386/kernel/smp.c */
+ #else
+     perf_sys_remote_sync();	/* Do it myself too... */
+ #endif
+ }
+ 
+ /*--------------------------------------------------------------------
+  *
+  */
+ static
+ int do_sys_perfop(int op, int arg1, int arg2) {
+     int i, j/*, retval=0 */;
+     int procid, counter;
+ 
+     if (!suser()) 
+ 	return -EPERM;
+ 
+     if ((perf_sys_flag) && (current->perf.options.is_sys_holder == 0))
+ 	return -EBUSY;
+ 
+     switch(op) {
+     case PERF_SYS_START:	/*----------------------------------*/
+ 	for (i=0; i < NR_CPUS; i++) perf_sys_conf[i][0] |= PERF_ENABLE;
+ 	sys_sync_counters();
+ 	return 0;
+     case PERF_SYS_STOP:		/*----------------------------------*/
+ 	for (i=0; i < NR_CPUS; i++) perf_sys_conf[i][0] &= ~PERF_ENABLE;
+ 	sys_sync_counters();
+ 	return 0;
+     case PERF_SYS_READ:		/*----------------------------------*/
+ 	counter = arg1 & 0xff;
+ 	if (counter < 0 || counter >= PERF_COUNTERS) return -EINVAL;
+ 	if (current->perf.options.sum_cpus == 0)
+ 	    {
+ 		procid  = (arg1 >> 8) & 0xff;
+ 		if (procid  < 0 || procid  >= NR_CPUS) return -EINVAL;
+ 		sys_sync_counters();
+ 		return copy_to_user((unsigned long long *) arg2,
+ 				    &perf_sys_counter[procid][counter],
+ 				    sizeof(unsigned long long)) ? -EFAULT : 0;
+ 	    }
+ 	else
+ 	    {
+ 		unsigned long long tmp_tot = 0;
+ 
+ 		sys_sync_counters();
+ 		for (i=0; i < NR_CPUS; i++) 
+ 		    tmp_tot += perf_sys_counter[i][counter];
+ 		return copy_to_user((unsigned long long *) arg2,
+ 				    &tmp_tot,
+ 				    sizeof(unsigned long long)) ? -EFAULT : 0;
+ 	    }
+     case PERF_SYS_RESET:	/*----------------------------------*/
+ 	for (j=0; j < NR_CPUS; j++) 
+ 	    for (i=0; i < PERF_COUNTERS; i++)
+ 		perf_sys_conf   [j][i] = 0;
+ 	sys_sync_counters();
+ 	for (j=0; j < NR_CPUS; j++) 
+ 	    for (i=0; i < PERF_COUNTERS; i++)
+ 		perf_sys_counter[j][i] = 0;
+ 	perf_sys_flag = 0;
+ 	current->perf.options.is_sys_holder = 0;
+ 	return 0;
+     case PERF_SYS_RESET_COUNTERS:	/*----------------------------------*/
+ 	sys_sync_counters();
+ 	for (j=0; j < NR_CPUS; j++) 
+ 	    for (i=0; i < PERF_COUNTERS; i++)
+ 		perf_sys_counter[j][i] = 0;
+ 	return 0;
+     case PERF_SYS_SET_CONFIG:	/*----------------------------------*/
+ 	perf_sys_flag = 1;
+ 	current->perf.options.is_sys_holder = 1;
+ 	procid  = (arg1 >> 8) & 0xff; counter = arg1 & 0xff;
+ 	if (procid  < 0 || procid  >= NR_CPUS ||
+ 	    counter < 0 || counter >= PERF_COUNTERS) return -EINVAL;
+ 	if ((arg2 & (PERF_OS|PERF_USR)) == 0) {
+ 	    arg2 |= PERF_USR;	/* Default mode = PERF_USR */
+ 	}
+ 	perf_sys_conf[procid][counter] =
+ 	  arg2 & (PERF_OS|PERF_USR|PERF_EVNT_MASK|PERF_UNIT_MASK);
+ 	sys_sync_counters();
+ 	return 0;
+     case PERF_SYS_GET_CONFIG:	/*----------------------------------*/
+ 	procid  = (arg1 >> 8) & 0xff;
+ 	counter = arg1 & 0xff;
+ 	if (procid  < 0 || procid  >= NR_CPUS ||
+ 	    counter < 0 || counter >= PERF_COUNTERS) return -EINVAL;
+ 	return put_user(perf_sys_conf[procid][counter] &
+ 			(PERF_OS|PERF_USR|PERF_EVNT_MASK|PERF_UNIT_MASK),
+ 			(int *)arg2);
+     case PERF_SYS_WRITE:	/*----------------------------------*/
+ 	procid  = (arg1 >> 8) & 0xff;
+ 	counter = arg1 & 0xff;
+ 	if (procid  < 0 || procid  >= NR_CPUS ||
+ 	    counter < 0 || counter >= PERF_COUNTERS) return -EINVAL;
+ 	sys_sync_counters();	/* Read the counters before writing in
+ 				 * order zero the hardware counters */
+ 	return copy_from_user(&perf_sys_counter[procid][counter],
+ 			      (unsigned long long *) arg2,
+ 			      sizeof(unsigned long long)) ? -EFAULT : 0;
+     default:
+ 	return -EINVAL;
+     }
+ }
+ 
+ /*--------------------------------------------------------------------
+  * do_user_perfop
+  *
+  *  The CPU counter registers are saved before calling this 
+  *  function and restored afterwards.
+  */
+ static
+ int do_user_perfop(int op, int arg1, int arg2) {
+     switch(op) {
+     case PERF_FASTREAD:
+ 	return copy_to_user((unsigned long long *) arg1,
+ 			    &current->perf.counter[0],
+ 			    PERF_COUNTERS*sizeof(unsigned long long)) 
+ 	    ? -EFAULT: 0;
+     case PERF_FASTWRITE:
+ 	return copy_from_user(&current->perf.counter[0],
+ 			      (unsigned long long *) arg1,
+ 			      PERF_COUNTERS*sizeof(unsigned long long)) 
+ 	    ? -EFAULT: 0;
+     case PERF_FASTCONFIG:	
+ 	return copy_from_user(&current->perf.conf[0], 
+ 			      (unsigned long *) arg1,
+ 			      PERF_COUNTERS*sizeof(int)) 
+ 	    ? -EFAULT: 0;
+     case PERF_RESET_COUNTERS:	/*----------------------------------*/
+ 	{
+ 	    int i;
+ 	    for (i=0; i < PERF_COUNTERS; i++) 
+ 		current->perf.counter[i] = 0;
+ 	    return 0;
+ 	}
+     case PERF_RESET:		/*----------------------------------*/
+ 	{
+ 	    int i;
+ 	    for (i=0; i < PERF_COUNTERS; i++) 
+ 		{
+ 		    current->perf.conf   [i] = 0;
+ 		    current->perf.counter[i] = 0;
+ 		}
+ 	    current->perf.options.do_children = 0;
+ 	    current->perf.options.sum_cpus = 0;
+ 	}
+ 	return 0;
+     case PERF_START:
+ 	current->perf.conf[0] |= PERF_ENABLE;
+ 	return 0;
+     case PERF_STOP:
+ 	current->perf.conf[0] &= ~PERF_ENABLE;
+ 	return 0;
+     case PERF_READ:
+ 	if (arg1 < 0 || arg1 >= PERF_COUNTERS) return -EINVAL;
+ 	return copy_to_user((unsigned long long *) arg2,
+ 			    &current->perf.counter[arg1],
+ 			    sizeof(unsigned long long)) ? -EFAULT: 0;
+     case PERF_SET_OPT:
+ 	{
+ 	    if (arg1 == PERF_DO_CHILDREN)
+ 		current->perf.options.do_children = (arg2 != 0);
+ 	    else if (arg1 == PERF_SUM_CPUS)
+ 		current->perf.options.sum_cpus = (arg2 != 0);
+ 	    else
+ 		return -EINVAL;
+ 	    return 0;
+ 	}
+     case PERF_GET_OPT:
+ 	{
+ 	    int tmp;
+ 
+ 	    if (arg1 == PERF_DO_CHILDREN)
+ 		tmp = (current->perf.options.do_children != 0);
+ 	    else if (arg1 == PERF_SUM_CPUS)
+ 		tmp = (current->perf.options.sum_cpus != 0);
+ 	    else
+ 		return -EINVAL;
+ 
+ 	    return put_user(tmp, (int *) arg2);
+ 	}
+     case PERF_SET_CONFIG:	/*----------------------------------*/
+ 	if (arg1 < 0 || arg1 >= PERF_COUNTERS) return -EINVAL;
+ 	if ((arg2 & (PERF_OS|PERF_USR)) == 0) 
+ 	    arg2 |= PERF_USR;	/* Default mode = PERF_USR */
+ 	current->perf.conf[arg1] =
+ 	    arg2 & (PERF_OS|PERF_USR|PERF_EVNT_MASK|PERF_UNIT_MASK);
+ 	current->perf.counter[arg1] = 0;
+ 	return 0;
+     case PERF_GET_CONFIG:
+ 	if (arg1 < 0 || arg1 >= PERF_COUNTERS) return -EINVAL;
+ 	return put_user(current->perf.conf[arg1] &
+ 			(PERF_OS|PERF_USR|PERF_EVNT_MASK|PERF_UNIT_MASK),
+ 			(int *) arg2);
+     case PERF_WRITE:
+ 	if (arg1 < 0 || arg1 >= PERF_COUNTERS) return -EINVAL;
+ 	return copy_from_user(&current->perf.counter[arg1], 
+ 			      (unsigned long long *) arg2,
+ 			      sizeof(unsigned long long)) ? -EFAULT : 0;
+     case PERF_WAIT:
+ 	{
+ 	    int retval;
+ 	    struct perf_wait_struct p;
+ 	    retval = copy_from_user(&p, (struct perf_wait_struct *)arg1,
+ 			    sizeof(struct perf_wait_struct)) ? -EFAULT : 0;
+ 	    if (retval) return retval;
+ 	    return do_wait(p.pid, p.status, p.options,p.rusage, p.counts);
+ 	}
+     default:
+ 	return -EINVAL;
+     }
+ }
+ 
+ /*
+  * sys_perf
+  *
+  * syscall for manipulation of performance counters.
+  */
+ asmlinkage int
+ sys_perf(int op, int arg1, int arg2) {
+     int retval;
+ 
+     lock_kernel();
+ 
+     switch(op) {
+     case PERF_FASTREAD:
+     case PERF_FASTWRITE:
+     case PERF_FASTCONFIG:
+     case PERF_START:
+     case PERF_STOP:
+     case PERF_RESET:		
+     case PERF_RESET_COUNTERS:  
+     case PERF_SET_CONFIG:
+     case PERF_GET_CONFIG:
+     case PERF_SET_OPT:
+     case PERF_GET_OPT:
+     case PERF_READ:
+     case PERF_WRITE:
+     case PERF_WAIT:
+ 	/* If someone is using the system counters or if our parent has locked our counters, then bail. */
+ 	if ((perf_sys_flag) || (current->perf.options.do_children > 1)) 
+ 	    {
+ 		unlock_kernel();
+ 		return -EBUSY; 
+ 	    }
+ 	perf_save(current->perf.conf, current->perf.counter, &current->perf.shadow_tsc);
+ 	retval = do_user_perfop(op, arg1, arg2);
+ 	perf_restore(current->perf.conf, &current->perf.shadow_tsc);	
+ 	unlock_kernel();
+ 	return retval;
+     case PERF_SYS_RESET:
+     case PERF_SYS_RESET_COUNTERS:
+     case PERF_SYS_SET_CONFIG:
+     case PERF_SYS_GET_CONFIG:
+     case PERF_SYS_START:
+     case PERF_SYS_STOP:
+     case PERF_SYS_READ:
+     case PERF_SYS_WRITE:
+ 	retval = do_sys_perfop(op, arg1, arg2);
+ 	unlock_kernel();
+ 	return retval;
+     case PERF_DEBUG:
+ 	{
+ #define load64(ll,l,h)  (l)=*((int *)&(ll));(h)=*(((int *)&(ll))+1);
+ 	    int hi0, lo0, hi1, lo1, hi2, lo2;
+ 	    printk("-[ %5d ] ------------------------------------\n",
+ 		   current->pid);
+ 	    printk("CONF   :         %08x         %08x          %08x\n",
+ 		   current->perf.conf[0], current->perf.conf[1], 
+ 		   current->perf.conf[2]);
+ 	    load64(current->perf.counter[0], lo0, hi0);
+ 	    load64(current->perf.counter[1], lo1, hi1);
+ 	    load64(current->perf.counter[2], lo2, hi2);
+ 	    printk("COUNTER: %08x%08x %08x%08x %08x%08x\n", 
+ 		   hi0, lo0, hi1, lo1, hi2, lo2);
+ 	    load64(current->perf.shadow_tsc, lo2, hi2);
+ 	    printk("SHADOW_TSC: %08x%08x\n", 
+ 		   hi2, lo2);
+ 	    printk("OPTIONS: do_children %d sum_cpus %d is_sys_holder %d\n",
+ 		   current->perf.options.do_children,
+ 		   current->perf.options.sum_cpus,
+ 		   current->perf.options.is_sys_holder);
+ 	}
+ 	unlock_kernel();
+ 	return 0;
+     default:
+ 	unlock_kernel();
+ 	return -EINVAL;
+     }
+ }
+ 
+ /*
+  * Local variables:
+  * c-basic-offset: 4
+  * End:
+  */
diff -crwN linux-2.2.16/arch/i386/kernel/smp.c linux-2.2.16-papi/arch/i386/kernel/smp.c
*** linux-2.2.16/arch/i386/kernel/smp.c	Wed Jun  7 17:26:42 2000
--- linux-2.2.16-papi/arch/i386/kernel/smp.c	Thu Nov 16 15:01:46 2000
***************
*** 41,46 ****
--- 41,49 ----
  #include <linux/init.h>
  #include <asm/mtrr.h>
  #include <asm/msr.h>
+ #ifdef CONFIG_PERF
+ #include <asm/perf.h>
+ #endif
  
  #include "irq.h"
  
***************
*** 123,129 ****
  volatile unsigned long syscall_count=0;			/* Number of times the processor holds the syscall lock	*/
  
  volatile unsigned long ipi_count;			/* Number of IPIs delivered				*/
! 
  const char lk_lockmsg[] = "lock from interrupt context at %p\n"; 
  
  int mp_bus_id_to_type [MAX_MP_BUSSES] = { -1, };
--- 126,134 ----
  volatile unsigned long syscall_count=0;			/* Number of times the processor holds the syscall lock	*/
  
  volatile unsigned long ipi_count;			/* Number of IPIs delivered				*/
! #ifdef CONFIG_PERF
! volatile unsigned long perf_sys_update_needed;
! #endif
  const char lk_lockmsg[] = "lock from interrupt context at %p\n"; 
  
  int mp_bus_id_to_type [MAX_MP_BUSSES] = { -1, };
***************
*** 1897,1902 ****
--- 1902,1952 ----
  {
  	send_IPI_allbutself(STOP_CPU_VECTOR);
  }
+ 
+ #ifdef CONFIG_PERF
+ /* Note this is basically just a copy of smp_flush_tlb (with most of
+  * the comments ripped out for size - see comments there if modifying)
+  * (Doing cli(); while waiting for procs to ACK scares me.  Also,
+  * worrying about "crossing" updates seems unnecessary if holding the
+  * kernel lock is a requirement for calling.  But, hey, I'm not an
+  * expert on this stuff.  I'll follow the example.) */
+ void smp_perf_sys_update(void)
+ {
+ 	int cpu = smp_processor_id();
+ 	int stuck;
+ 	unsigned long flags;
+ 
+ 	if (cpu_online_map) {
+ 		perf_sys_update_needed = cpu_online_map;
+ 
+ 		__save_flags(flags);
+ 		__cli();
+ 
+ 		send_IPI_allbutself(PERF_SYS_UPDATE_VECTOR);
+ 
+ 		stuck = 50000000;
+ 		while (perf_sys_update_needed) {
+ 			if (test_bit(cpu, &perf_sys_update_needed))
+ 			clear_bit(cpu, &perf_sys_update_needed);
+ 			--stuck;
+ 			if (!stuck) {
+ 				printk("stuck on perf update IPI wait (CPU#%d)\n",cpu);
+ 				break;
+ 			}
+ 		}
+ 		__restore_flags(flags);
+ 	}
+ 	perf_sys_remote_sync();
+ }
+ 
+ asmlinkage void smp_perf_sys_update_interrupt(void)
+ {
+ 	if (test_and_clear_bit(smp_processor_id(), &perf_sys_update_needed))
+ 		perf_sys_remote_sync();
+ 
+ 	ack_APIC_irq();
+ }
+ #endif
  
  /* Structure and data for smp_call_function(). This is designed to minimise
   * static memory requirements. It also looks cleaner.
diff -crwN linux-2.2.16/include/asm/perf.h linux-2.2.16-papi/include/asm/perf.h
*** linux-2.2.16/include/asm/perf.h	Wed Dec 31 19:00:00 1969
--- linux-2.2.16-papi/include/asm/perf.h	Thu Nov 16 15:01:46 2000
***************
*** 0 ****
--- 1,235 ----
+ #ifndef _I386_PERF_H_
+ #define _I386_PERF_H_
+ 
+ /* Per process cycle counter is third counter. */
+ #define PERF_CYCLES                 0x1
+ 
+ /* Countable things: (For PPro) */
+ /* Data Cache Unit (DCU) */
+ #define PERF_DATA_MEM_REFS          0x43
+ #define PERF_DCU_LINES_IN           0x45
+ #define PERF_DCU_M_LINES_IN         0x46
+ #define PERF_DCU_M_LINES_OUT        0x47
+ #define PERF_DCU_MISS_STANDING      0x48
+ 
+ /* Instruction Fetch Unit (IFU) */
+ #define PERF_IFU_IFETCH             0x80
+ #define PERF_IFU_IFETCH_MISS        0x81
+ #define PERF_ITLB_MISS              0x85
+ #define PERF_IFU_MEM_STALL          0x86
+ #define PERF_ILD_STALL              0x87
+ 
+ /* L2 Cache */
+ #define PERF_L2_IFETCH              0x28  /* MESI */
+ #define PERF_L2_LD                  0x29  /* MESI */
+ #define PERF_L2_ST                  0x2A  /* MESI */
+ #define PERF_L2_LINES_IN            0x24
+ #define PERF_L2_LINES_OUT           0x26
+ #define PERF_L2_LINES_INM           0x25
+ #define PERF_L2_LINES_OUTM          0x27
+ #define PERF_L2_RQSTS               0x2E  /* MESI */
+ #define PERF_L2_ADS                 0x21
+ #define PERF_L2_DBUS_BUSY           0x22
+ #define PERF_L2_DBUS_BUSY_RD        0x23
+ 
+ /* External Bus Logic */
+ #define PERF_BUS_DRDY_CLOCKS        0x62
+ #define PERF_BUS_LOCK_CLOCKS        0x63
+ #define PERF_BUS_REQ_OUTSTANDING    0x60
+ #define PERF_BUS_TRAN_BRD           0x65
+ #define PERF_BUS_TRAN_RFO           0x66
+ #define PERF_BUS_TRANS_WB           0x67
+ #define PERF_BUS_TRAN_IFETCH        0x68
+ #define PERF_BUS_TRAN_INVAL         0x69
+ #define PERF_BUS_TRAN_PWR           0x6A
+ #define PERF_BUS_TRAN_P             0x6B
+ #define PERF_BUS_TRANS_IO           0x6C
+ #define PERF_BUS_TRAN_DEF           0x6D
+ #define PERF_BUS_TRAN_BURST         0x6E
+ #define PERF_BUS_TRAN_ANY           0x70
+ #define PERF_BUS_TRAN_MEM           0x6F
+ #define PERF_BUS_DATA_RCV           0x64
+ #define PERF_BUS_BNR_DRV            0x61
+ #define PERF_BUS_HIT_DRV            0x7A
+ #define PERF_BUS_HITM_DRV           0x7B
+ #define PERF_BUS_SNOOP_STALL        0x7E
+ 
+ /* Floating point unit */
+ #define PERF_FLOPS                  0xC1
+ #define PERF_FP_COMP_OPS_EXE        0x10
+ #define PERF_FP_ASSIST              0x11
+ #define PERF_MUL                    0x12
+ #define PERF_DIV                    0x13
+ #define PERF_CYCLES_DIV_BUSY        0x14
+ 
+ /* Memory Ordering */
+ #define PERF_LD_BLOCK               0x03
+ #define PERF_SB_DRAINS              0x04
+ #define PERF_MISALIGN_MEM_REF       0x05
+ 
+ /* Instruction Decoding and Retirement */ 
+ #define PERF_INST_RETIRED           0xC0
+ #define PERF_UOPS_RETIRED           0xC2
+ #define PERF_INST_DECODER           0xD0
+ 
+ /* Interrupts */
+ #define PERF_HW_INT_RX              0xC8
+ #define PERF_CYCLES_INST_MASKED     0xC6
+ #define PERF_CYCLES_INT_PENDING_AND_MASKED 0xC7
+ 
+ /* Branches */
+ #define PERF_BR_INST_RETIRED        0xC4
+ #define PERF_BR_MISS_PRED_RETIRED   0xC5
+ #define PERF_BR_TAKEN_RETIRED       0xC9
+ #define PERF_BR_MISS_PRED_TAKEN_RET 0xCA
+ #define PERF_BR_INST_DECODED        0xE0
+ #define PERF_BR_BTB_MISSES          0xE2
+ #define PERF_BR_BOGUS               0xE4
+ #define PERF_BACLEARS               0xE6
+ 
+ /* Stalls */
+ #define PERF_RESOURCE_STALLS        0xA2
+ #define PERF_PARTIAL_RAT_STALLS     0xD2
+ 
+ /* Segment Register Loads */
+ #define PERF_SEGMENT_REG_LOADS      0x06
+ 
+ /* Clocks */
+ #define PERF_CPU_CLK_UNHALTED       0x79
+ 
+ /* Unit mask flags: */
+ #define PERF_SELF                 0x0000
+ #define PERF_ANY                  0x2000
+ #define PERF_CACHE_M              0x0800
+ #define PERF_CACHE_E              0x0400
+ #define PERF_CACHE_S              0x0200
+ #define PERF_CACHE_I              0x0100
+ #define PERF_CACHE_ALL            0x0F00
+ 
+ /*--------------------------------------------------------*/
+ /* Bit masks for configuration fields */
+ #define PERF_CTR_MASK          0xFF000000
+ #define PERF_INV_CTR_MASK      0x00800000
+ #define PERF_ENABLE            0x00400000
+ #define PERF_INT_ENABLE        0x00100000
+ #define PERF_PIN_CONTROL       0x00080000
+ #define PERF_EDGE_DETECT       0x00040000
+ #define PERF_OS                0x00020000
+ #define PERF_USR               0x00010000
+ #define PERF_UNIT_MASK         0x0000FF00
+ #define PERF_EVNT_MASK         0x000000FF
+ 
+ /* System calls */
+ 
+ #define PERF_RESET          0
+ #define PERF_SET_CONFIG     1
+ #define PERF_GET_CONFIG     2
+ #define PERF_START          3
+ #define PERF_STOP           4
+ #define PERF_READ           5
+ #define PERF_WRITE          6
+ #define PERF_WAIT           7
+ #define PERF_DEBUG          8
+ #define PERF_SET_OPT        9
+ #define PERF_GET_OPT        10
+ #define PERF_RESET_COUNTERS 11
+ #define PERF_FASTWRITE      12
+ #define PERF_FASTREAD       13
+ #define PERF_FASTCONFIG     14
+ #define PERF_SYS_RESET      20
+ #define PERF_SYS_SET_CONFIG 21
+ #define PERF_SYS_GET_CONFIG 22
+ #define PERF_SYS_START      23
+ #define PERF_SYS_STOP       24
+ #define PERF_SYS_READ       25
+ #define PERF_SYS_WRITE      26
+ #define PERF_SYS_RESET_COUNTERS 27
+ 
+ /* Options */
+ 
+ #define PERF_DO_CHILDREN    1
+ #define PERF_SUM_CPUS       2
+ 
+ /* Number of performance counters */
+ /* Counter 2 is always the virtual cycle counter. */
+ 
+ #define PERF_COUNTERS 3
+ 
+ struct perf_wait_struct { 
+   pid_t  pid;
+   int   *status;
+   int    options;
+   struct rusage *rusage;
+   unsigned long long *counts;
+ };
+ 
+ static inline unsigned long long perf_get_cycles (void)
+ {
+ 	unsigned long long ret;
+         __asm__ __volatile__("rdtsc"
+ 			    : "=A" (ret)
+ 			    : /* no inputs */);
+         return ret;
+ }
+ 
+ #ifdef __KERNEL__
+ 
+ #define MSR_PERFCTR0 0x00C1
+ #define MSR_PERFCTR1 0x00C2
+ #define MSR_EVNTSEL0 0x0186
+ #define MSR_EVNTSEL1 0x0187
+ 
+ struct proc_perf_opt_t {
+     unsigned int do_children:16;
+     unsigned int sum_cpus:8;
+     unsigned int is_sys_holder:8;
+ };
+ 
+ struct proc_perf_t {
+     struct proc_perf_opt_t options;
+     unsigned int       conf   [PERF_COUNTERS];
+     unsigned long long counter[PERF_COUNTERS]; /* Last one is always the TSC for this process */
+     unsigned long long shadow_tsc;
+ };
+ 
+ #define PROC_PERF_INIT {{0,},{0,},{0,},0}
+ 
+ extern volatile int perf_sys_flag;
+ 
+ static inline
+ unsigned long long perf_rdmsr(unsigned int msr) {
+     unsigned long long ret;
+     __asm__ __volatile__("rdmsr"
+ 			 : "=A" (ret)
+ 			 : "c" (msr));
+     return ret;
+ }
+ 
+ static inline
+ void perf_wrmsr(unsigned int msr,unsigned long long val) {
+     __asm__ __volatile__("wrmsr"
+ 			 : /* no Outputs */
+ 			 : "c" (msr), "A" (val));
+ }
+ 
+ static inline
+ void perf_restore(unsigned int *config, unsigned long long *shadow_tsc) {
+     perf_wrmsr(MSR_EVNTSEL0, config[0]);
+     perf_wrmsr(MSR_PERFCTR0, 0);
+     perf_wrmsr(MSR_EVNTSEL0+1, config[1]);
+     perf_wrmsr(MSR_PERFCTR0+1, 0);
+     *shadow_tsc = perf_get_cycles();
+ }
+ 
+ static inline
+ void perf_save(unsigned int *config, unsigned long long *counter, unsigned long long *shadow_tsc) {
+     counter[0] += (perf_rdmsr(MSR_PERFCTR0)&((1ULL<<40)-1));
+     counter[1] += (perf_rdmsr(MSR_PERFCTR0+1)&((1ULL<<40)-1));
+     if (config[2])
+       counter[2] += perf_get_cycles() - *shadow_tsc;
+ }
+ 
+ void perf_sys_remote_sync(void);
+ void smp_perf_sys_update(void);
+ #endif
+ #endif
diff -crwN linux-2.2.16/include/asm/unistd.h linux-2.2.16-papi/include/asm/unistd.h
*** linux-2.2.16/include/asm/unistd.h	Wed Jul 12 12:55:49 2000
--- linux-2.2.16-papi/include/asm/unistd.h	Thu Nov 16 15:01:46 2000
***************
*** 202,207 ****
--- 202,208 ----
  #define __NR_stat64		195
  #define __NR_lstat64		196
  #define __NR_fstat64		197
+ #define __NR_perf		252
  
  /* user-visible error numbers are in the range -1 - -124: see <asm-i386/errno.h> */
  
diff -crwN linux-2.2.16/include/asm-i386/perf.h linux-2.2.16-papi/include/asm-i386/perf.h
*** linux-2.2.16/include/asm-i386/perf.h	Wed Dec 31 19:00:00 1969
--- linux-2.2.16-papi/include/asm-i386/perf.h	Thu Nov 16 15:01:46 2000
***************
*** 0 ****
--- 1,235 ----
+ #ifndef _I386_PERF_H_
+ #define _I386_PERF_H_
+ 
+ /* Per process cycle counter is third counter. */
+ #define PERF_CYCLES                 0x1
+ 
+ /* Countable things: (For PPro) */
+ /* Data Cache Unit (DCU) */
+ #define PERF_DATA_MEM_REFS          0x43
+ #define PERF_DCU_LINES_IN           0x45
+ #define PERF_DCU_M_LINES_IN         0x46
+ #define PERF_DCU_M_LINES_OUT        0x47
+ #define PERF_DCU_MISS_STANDING      0x48
+ 
+ /* Instruction Fetch Unit (IFU) */
+ #define PERF_IFU_IFETCH             0x80
+ #define PERF_IFU_IFETCH_MISS        0x81
+ #define PERF_ITLB_MISS              0x85
+ #define PERF_IFU_MEM_STALL          0x86
+ #define PERF_ILD_STALL              0x87
+ 
+ /* L2 Cache */
+ #define PERF_L2_IFETCH              0x28  /* MESI */
+ #define PERF_L2_LD                  0x29  /* MESI */
+ #define PERF_L2_ST                  0x2A  /* MESI */
+ #define PERF_L2_LINES_IN            0x24
+ #define PERF_L2_LINES_OUT           0x26
+ #define PERF_L2_LINES_INM           0x25
+ #define PERF_L2_LINES_OUTM          0x27
+ #define PERF_L2_RQSTS               0x2E  /* MESI */
+ #define PERF_L2_ADS                 0x21
+ #define PERF_L2_DBUS_BUSY           0x22
+ #define PERF_L2_DBUS_BUSY_RD        0x23
+ 
+ /* External Bus Logic */
+ #define PERF_BUS_DRDY_CLOCKS        0x62
+ #define PERF_BUS_LOCK_CLOCKS        0x63
+ #define PERF_BUS_REQ_OUTSTANDING    0x60
+ #define PERF_BUS_TRAN_BRD           0x65
+ #define PERF_BUS_TRAN_RFO           0x66
+ #define PERF_BUS_TRANS_WB           0x67
+ #define PERF_BUS_TRAN_IFETCH        0x68
+ #define PERF_BUS_TRAN_INVAL         0x69
+ #define PERF_BUS_TRAN_PWR           0x6A
+ #define PERF_BUS_TRAN_P             0x6B
+ #define PERF_BUS_TRANS_IO           0x6C
+ #define PERF_BUS_TRAN_DEF           0x6D
+ #define PERF_BUS_TRAN_BURST         0x6E
+ #define PERF_BUS_TRAN_ANY           0x70
+ #define PERF_BUS_TRAN_MEM           0x6F
+ #define PERF_BUS_DATA_RCV           0x64
+ #define PERF_BUS_BNR_DRV            0x61
+ #define PERF_BUS_HIT_DRV            0x7A
+ #define PERF_BUS_HITM_DRV           0x7B
+ #define PERF_BUS_SNOOP_STALL        0x7E
+ 
+ /* Floating point unit */
+ #define PERF_FLOPS                  0xC1
+ #define PERF_FP_COMP_OPS_EXE        0x10
+ #define PERF_FP_ASSIST              0x11
+ #define PERF_MUL                    0x12
+ #define PERF_DIV                    0x13
+ #define PERF_CYCLES_DIV_BUSY        0x14
+ 
+ /* Memory Ordering */
+ #define PERF_LD_BLOCK               0x03
+ #define PERF_SB_DRAINS              0x04
+ #define PERF_MISALIGN_MEM_REF       0x05
+ 
+ /* Instruction Decoding and Retirement */ 
+ #define PERF_INST_RETIRED           0xC0
+ #define PERF_UOPS_RETIRED           0xC2
+ #define PERF_INST_DECODER           0xD0
+ 
+ /* Interrupts */
+ #define PERF_HW_INT_RX              0xC8
+ #define PERF_CYCLES_INST_MASKED     0xC6
+ #define PERF_CYCLES_INT_PENDING_AND_MASKED 0xC7
+ 
+ /* Branches */
+ #define PERF_BR_INST_RETIRED        0xC4
+ #define PERF_BR_MISS_PRED_RETIRED   0xC5
+ #define PERF_BR_TAKEN_RETIRED       0xC9
+ #define PERF_BR_MISS_PRED_TAKEN_RET 0xCA
+ #define PERF_BR_INST_DECODED        0xE0
+ #define PERF_BR_BTB_MISSES          0xE2
+ #define PERF_BR_BOGUS               0xE4
+ #define PERF_BACLEARS               0xE6
+ 
+ /* Stalls */
+ #define PERF_RESOURCE_STALLS        0xA2
+ #define PERF_PARTIAL_RAT_STALLS     0xD2
+ 
+ /* Segment Register Loads */
+ #define PERF_SEGMENT_REG_LOADS      0x06
+ 
+ /* Clocks */
+ #define PERF_CPU_CLK_UNHALTED       0x79
+ 
+ /* Unit mask flags: */
+ #define PERF_SELF                 0x0000
+ #define PERF_ANY                  0x2000
+ #define PERF_CACHE_M              0x0800
+ #define PERF_CACHE_E              0x0400
+ #define PERF_CACHE_S              0x0200
+ #define PERF_CACHE_I              0x0100
+ #define PERF_CACHE_ALL            0x0F00
+ 
+ /*--------------------------------------------------------*/
+ /* Bit masks for configuration fields */
+ #define PERF_CTR_MASK          0xFF000000
+ #define PERF_INV_CTR_MASK      0x00800000
+ #define PERF_ENABLE            0x00400000
+ #define PERF_INT_ENABLE        0x00100000
+ #define PERF_PIN_CONTROL       0x00080000
+ #define PERF_EDGE_DETECT       0x00040000
+ #define PERF_OS                0x00020000
+ #define PERF_USR               0x00010000
+ #define PERF_UNIT_MASK         0x0000FF00
+ #define PERF_EVNT_MASK         0x000000FF
+ 
+ /* System calls */
+ 
+ #define PERF_RESET          0
+ #define PERF_SET_CONFIG     1
+ #define PERF_GET_CONFIG     2
+ #define PERF_START          3
+ #define PERF_STOP           4
+ #define PERF_READ           5
+ #define PERF_WRITE          6
+ #define PERF_WAIT           7
+ #define PERF_DEBUG          8
+ #define PERF_SET_OPT        9
+ #define PERF_GET_OPT        10
+ #define PERF_RESET_COUNTERS 11
+ #define PERF_FASTWRITE      12
+ #define PERF_FASTREAD       13
+ #define PERF_FASTCONFIG     14
+ #define PERF_SYS_RESET      20
+ #define PERF_SYS_SET_CONFIG 21
+ #define PERF_SYS_GET_CONFIG 22
+ #define PERF_SYS_START      23
+ #define PERF_SYS_STOP       24
+ #define PERF_SYS_READ       25
+ #define PERF_SYS_WRITE      26
+ #define PERF_SYS_RESET_COUNTERS 27
+ 
+ /* Options */
+ 
+ #define PERF_DO_CHILDREN    1
+ #define PERF_SUM_CPUS       2
+ 
+ /* Number of performance counters */
+ /* Counter 2 is always the virtual cycle counter. */
+ 
+ #define PERF_COUNTERS 3
+ 
+ struct perf_wait_struct { 
+   pid_t  pid;
+   int   *status;
+   int    options;
+   struct rusage *rusage;
+   unsigned long long *counts;
+ };
+ 
+ static inline unsigned long long perf_get_cycles (void)
+ {
+ 	unsigned long long ret;
+         __asm__ __volatile__("rdtsc"
+ 			    : "=A" (ret)
+ 			    : /* no inputs */);
+         return ret;
+ }
+ 
+ #ifdef __KERNEL__
+ 
+ #define MSR_PERFCTR0 0x00C1
+ #define MSR_PERFCTR1 0x00C2
+ #define MSR_EVNTSEL0 0x0186
+ #define MSR_EVNTSEL1 0x0187
+ 
+ struct proc_perf_opt_t {
+     unsigned int do_children:16;
+     unsigned int sum_cpus:8;
+     unsigned int is_sys_holder:8;
+ };
+ 
+ struct proc_perf_t {
+     struct proc_perf_opt_t options;
+     unsigned int       conf   [PERF_COUNTERS];
+     unsigned long long counter[PERF_COUNTERS]; /* Last one is always the TSC for this process */
+     unsigned long long shadow_tsc;
+ };
+ 
+ #define PROC_PERF_INIT {{0,},{0,},{0,},0}
+ 
+ extern volatile int perf_sys_flag;
+ 
+ static inline
+ unsigned long long perf_rdmsr(unsigned int msr) {
+     unsigned long long ret;
+     __asm__ __volatile__("rdmsr"
+ 			 : "=A" (ret)
+ 			 : "c" (msr));
+     return ret;
+ }
+ 
+ static inline
+ void perf_wrmsr(unsigned int msr,unsigned long long val) {
+     __asm__ __volatile__("wrmsr"
+ 			 : /* no Outputs */
+ 			 : "c" (msr), "A" (val));
+ }
+ 
+ static inline
+ void perf_restore(unsigned int *config, unsigned long long *shadow_tsc) {
+     perf_wrmsr(MSR_EVNTSEL0, config[0]);
+     perf_wrmsr(MSR_PERFCTR0, 0);
+     perf_wrmsr(MSR_EVNTSEL0+1, config[1]);
+     perf_wrmsr(MSR_PERFCTR0+1, 0);
+     *shadow_tsc = perf_get_cycles();
+ }
+ 
+ static inline
+ void perf_save(unsigned int *config, unsigned long long *counter, unsigned long long *shadow_tsc) {
+     counter[0] += (perf_rdmsr(MSR_PERFCTR0)&((1ULL<<40)-1));
+     counter[1] += (perf_rdmsr(MSR_PERFCTR0+1)&((1ULL<<40)-1));
+     if (config[2])
+       counter[2] += perf_get_cycles() - *shadow_tsc;
+ }
+ 
+ void perf_sys_remote_sync(void);
+ void smp_perf_sys_update(void);
+ #endif
+ #endif
diff -crwN linux-2.2.16/include/asm-i386/unistd.h linux-2.2.16-papi/include/asm-i386/unistd.h
*** linux-2.2.16/include/asm-i386/unistd.h	Wed Jul 12 12:55:49 2000
--- linux-2.2.16-papi/include/asm-i386/unistd.h	Thu Nov 16 15:01:46 2000
***************
*** 202,207 ****
--- 202,208 ----
  #define __NR_stat64		195
  #define __NR_lstat64		196
  #define __NR_fstat64		197
+ #define __NR_perf		252
  
  /* user-visible error numbers are in the range -1 - -124: see <asm-i386/errno.h> */
  
diff -crwN linux-2.2.16/include/linux/sched.h linux-2.2.16-papi/include/linux/sched.h
*** linux-2.2.16/include/linux/sched.h	Wed Jul 12 13:24:00 2000
--- linux-2.2.16-papi/include/linux/sched.h	Thu Nov 16 15:01:46 2000
***************
*** 23,28 ****
--- 23,32 ----
  #include <linux/signal.h>
  #include <linux/securebits.h>
  
+ #ifdef CONFIG_PERF
+ #include <asm/perf.h>
+ #endif
+ 
  /*
   * cloning flags:
   */
***************
*** 332,337 ****
--- 336,346 ----
  
  /* oom handling */
  	int oom_kill_try;
+ 
+ /* performance monitoring */
+ #ifdef CONFIG_PERF
+       struct proc_perf_t perf;
+ #endif
  };
  
  /*
***************
*** 365,370 ****
--- 374,384 ----
   *  INIT_TASK is used to set up the first task table, touch at
   * your own risk!. Base=0, limit=0x1fffff (=2MB)
   */
+ #ifdef CONFIG_PERF
+ #define PERF_INIT PROC_PERF_INIT,
+ #else
+ #define PERF_INIT
+ #endif
  #define INIT_TASK \
  /* state etc */	{ 0,0,0,KERNEL_DS,&default_exec_domain,0, \
  /* counter */	DEF_PRIORITY,DEF_PRIORITY,0, \
***************
*** 400,405 ****
--- 414,420 ----
  /* signals */	SPIN_LOCK_UNLOCKED, &init_signals, {{0}}, {{0}}, NULL, &init_task.sigqueue, 0, 0, \
  /* exec cts */	0,0, \
  /* oom */	0, \
+ /* perf */      PERF_INIT \
  }
  
  union task_union {
diff -crwN linux-2.2.16/kernel/exit.c linux-2.2.16-papi/kernel/exit.c
*** linux-2.2.16/kernel/exit.c	Tue Jan  4 13:12:25 2000
--- linux-2.2.16-papi/kernel/exit.c	Thu Nov 16 15:01:46 2000
***************
*** 18,23 ****
--- 18,27 ----
  #include <asm/pgtable.h>
  #include <asm/mmu_context.h>
  
+ #ifdef CONFIG_PERF
+ #include <asm/perf.h>
+ #endif
+ 
  extern void sem_exit (void);
  extern struct task_struct *child_reaper;
  
***************
*** 395,400 ****
--- 399,408 ----
  #if CONFIG_AP1000
  	exit_msc(tsk);
  #endif
+ #ifdef CONFIG_PERF
+ 	if (tsk->perf.options.is_sys_holder)
+ 	  perf_sys_flag = 0;
+ #endif
  	__exit_files(tsk);
  	__exit_fs(tsk);
  	__exit_sighand(tsk);
***************
*** 431,437 ****
  	do_exit((error_code&0xff)<<8);
  }
  
! asmlinkage int sys_wait4(pid_t pid,unsigned int * stat_addr, int options, struct rusage * ru)
  {
  	int flag, retval;
  	struct wait_queue wait = { current, NULL };
--- 439,449 ----
  	do_exit((error_code&0xff)<<8);
  }
  
! int do_wait(pid_t pid,unsigned int * stat_addr, int options, struct rusage * ru
! #ifdef CONFIG_PERF
! 	    , unsigned long long *perf_counts
! #endif
! 	    )
  {
  	int flag, retval;
  	struct wait_queue wait = { current, NULL };
***************
*** 476,481 ****
--- 488,504 ----
  				retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0; 
  				if (!retval && stat_addr) 
  					retval = put_user((p->exit_code << 8) | 0x7f, stat_addr);
+ #ifdef CONFIG_PERF
+ 				if (p->perf.options.do_children)
+ 				  {
+ 				    current->perf.counter[0] += p->perf.counter[0];
+ 				    current->perf.counter[1] += p->perf.counter[1];
+ 				    current->perf.counter[2] += p->perf.counter[2]; 
+ 				  }
+ 
+ 				if (!retval && perf_counts)
+ 					retval = copy_to_user(perf_counts,p->perf.counter,sizeof(unsigned long long)*PERF_COUNTERS) ? -EFAULT : 0;
+ #endif
  				if (!retval) {
  					p->exit_code = 0;
  					retval = p->pid;
***************
*** 488,493 ****
--- 511,527 ----
  				retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
  				if (!retval && stat_addr)
  					retval = put_user(p->exit_code, stat_addr);
+ #ifdef CONFIG_PERF
+ 				if (p->perf.options.do_children)
+ 				  {
+ 				    current->perf.counter[0] += p->perf.counter[0];
+ 				    current->perf.counter[1] += p->perf.counter[1];
+ 				    current->perf.counter[2] += p->perf.counter[2]; 
+ 				  }
+ 
+ 				if (!retval && perf_counts)
+ 					retval = copy_to_user(perf_counts,p->perf.counter,sizeof(unsigned long long)*PERF_COUNTERS) ? -EFAULT : 0;
+ #endif
  				if (retval)
  					goto end_wait4; 
  				retval = p->pid;
***************
*** 524,529 ****
--- 558,572 ----
  	remove_wait_queue(&current->wait_chldexit,&wait);
  	current->state = TASK_RUNNING;
  	return retval;
+ }
+ 
+ asmlinkage int sys_wait4(pid_t pid,unsigned int * stat_addr, int options, struct rusage * ru)
+ {
+ 	return do_wait(pid, stat_addr, options, ru
+ #ifdef CONFIG_PERF
+ 		, 0
+ #endif
+ 		);
  }
  
  #ifndef __alpha__
diff -crwN linux-2.2.16/kernel/fork.c linux-2.2.16-papi/kernel/fork.c
*** linux-2.2.16/kernel/fork.c	Tue Oct 26 20:53:42 1999
--- linux-2.2.16-papi/kernel/fork.c	Thu Nov 16 15:01:46 2000
***************
*** 22,27 ****
--- 22,31 ----
  #include <asm/mmu_context.h>
  #include <asm/uaccess.h>
  
+ #ifdef CONFIG_PERF
+ #include <asm/perf.h>
+ #endif
+ 
  /* The idle tasks do not count.. */
  int nr_tasks=0;
  int nr_running=0;
***************
*** 664,669 ****
--- 668,703 ----
  #endif
  	p->lock_depth = -1;		/* -1 = no lock */
  	p->start_time = jiffies;
+ #ifdef CONFIG_PERF
+ 	{
+ 		int i;
+ 
+ 		if (current->perf.options.do_children)
+ 		  {
+ 		    p->perf.options    = current->perf.options;
+ 		    p->perf.shadow_tsc = 0;
+ 		    if (current->perf.options.do_children == 1)
+ 		      p->perf.options.do_children = 2;
+ 		    for (i=0; i < PERF_COUNTERS; i++) 
+ 		      {
+ 			p->perf.conf[i]    = current->perf.conf[i];
+ 			p->perf.counter[i] = 0;
+ 		      }
+ 		  }
+ 		else
+ 		  {
+ 		    p->perf.options.do_children = 0;
+ 		    p->perf.options.is_sys_holder = 0;
+ 		    p->perf.options.sum_cpus = 0;
+ 		    p->perf.shadow_tsc = 0;
+ 		    for (i=0; i < PERF_COUNTERS; i++) 
+ 		      {
+ 			p->perf.conf[i]    = 0;
+ 			p->perf.counter[i] = 0;
+ 		      }
+ 		  }
+ 	}
+ #endif
  
  	retval = -ENOMEM;
  	/* copy all the process information */
diff -crwN linux-2.2.16/kernel/sched.c linux-2.2.16-papi/kernel/sched.c
*** linux-2.2.16/kernel/sched.c	Wed Jun  7 17:26:44 2000
--- linux-2.2.16-papi/kernel/sched.c	Thu Nov 16 15:01:46 2000
***************
*** 37,43 ****
  #include <asm/pgtable.h>
  #include <asm/mmu_context.h>
  #include <asm/semaphore-helper.h>
! 
  #include <linux/timex.h>
  
  /*
--- 37,45 ----
  #include <asm/pgtable.h>
  #include <asm/mmu_context.h>
  #include <asm/semaphore-helper.h>
! #ifdef CONFIG_PERF
! #include <asm/perf.h>
! #endif
  #include <linux/timex.h>
  
  /*
***************
*** 826,831 ****
--- 828,845 ----
  #endif /* __SMP__ */
  
  	kstat.context_swtch++;
+ #ifdef CONFIG_PERF
+ 		{
+ 			unsigned long flags;
+ 			save_flags(flags);
+ 			cli();
+ 			if (!perf_sys_flag) {
+ 				perf_save(prev->perf.conf, prev->perf.counter, &(prev->perf.shadow_tsc));
+ 				perf_restore(next->perf.conf, &(next->perf.shadow_tsc));
+ 			}
+ 			restore_flags(flags);
+ 		}
+ #endif
  	get_mmu_context(next);
  	switch_to(prev, next, prev);
  	__schedule_tail(prev);
