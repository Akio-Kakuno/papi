Title: Notes for the Cray X1

Counters:
The Cray X-1 has 4 P-Chip counters per MSP (1 for each SSP), in addition there are
4 E-Chip counters and 16 M-Chip counters.  Currently, PAPI only supports 1 value
for each counter, and thus we send back an aggregate of all the counters.  For
P-Chips it may be desirable to see each SSP counter seperately, as is the case
if a MPI job runs 4 processes on a single MSP and we are looking into ways of
providing this for the user, but with the current implementation you can only
obtain an aggregate. This of course only affects MSP applications.
It is unclear which E-Chip and M-Chip counters should be read depending on the
MSP the process is running on or if all should be aggregated.  We are still
examining the correlation between the process and MSP/ssp it is running on and
the E-Chip/M-Chip counters.  Thus the interface may end up changing to aggregate
certain counters or return all the values as we wish to do for the P-chip.  But
the user should be aware that all counters are aggregated currently.

Overhead:
Unlike most other platforms, the overhead on the Cray X1 is dependant on what
events you are monitoring.  Each chip type (P/M/E) requires its own ioctl call
to start/stop and read the hardware counters.  Because of this if you monitor
events from multiple chips your overhead will double if monitoring events from
2 chip types or triple if monitoring from 3 chips types as most of the overhead
comes from the ioctl calls.

Overflow:
The Cray X-1 has hardware overflow support for the P-Chip, however the M-Chip 
and E-Chip do not support hardware overflow.  Because of this, when overflowing
on events, if only P-Chip events are being overflowed on then we use hardware
overflow.  But if we are overflowing on M or E-Chip events (alone, together, or in
combination with P-Chip events) then we use software overflowing.

Profiling:
Hardware profiling is not available on the Cray X-1 and so we use overflows and 
return the pc to do profiling.  Because of this we are either using hardware
or software overflowing.  Read the Overflow section to determine which will be
used.

Multiplexing:
Only software multiplexing is available, currently software multiplexing only 
monitors 1 event per time slice, and so multiplexing is discouraged for the
X1 since there are 64 possible counters.

Native Events:
For *all* available native event names, run native_avail under the
ctests subdirectory.  

For more information on the native events SEE:  man counters

Shared Objects:
Shared Objects are not supported on the Cray X1 and thus a shared library can not be
built and we don't support any shared library information.

Cache Information:
Currently this is no supported, but will be added in future releases.

Timer latencies:
The PAPI_get_real_usec uses the highest resolution timer available on the Cray X1 which
is currently 10 ms.

Other Issues:
The user should be aware that we do not stop the counters when reading the hardware 
counters as this would require 2 additional ioctl calls per chips (IE worse case
scenario 9 ioctl calls to read all chips versus the 3 ioctl calls to read the
chips if we dont' stop/start the counters)  Because of this the PAPI library will
affect some of the counts.  Since everything is aggregate we loop through the
P-Chips 32 counters and add them up and then copy that information to the users
array.  Each is a long long and thus this will effect the instruction count.  
The E and M chip are already aggregated from the hardware and so the PAPI interface
only has to copy the results.
hardware and thus PAPI only has to cop

