The PAPI NVML component provides an interface to the NVidia Management Library (nvml, libnvidia-ml).

In versions 8 and later part of the CUDA Toolkit, The NVIDIA
Management Library is no longer a separate download and is installed with
CUDA. On Linux/x86 platforms, it is often found in
/usr/lib64/nvidia/libnvidia-ml.so

NOTE for ICL staff: On Summit, it is found in /usr/lib64/libnvidia-ml.so.1
However, this is a problem, the name (with a version number on it) is not the
default.  In order to open a lib of a different name, you must change the
Rules.nvml file, and specify the exact name to use with the line: 
LIBCFLAGS += -DNVML_LIBNAME="libnvidia-ml.so.1" 
If that library is not on a standard path (/usr/lib64 is however), the path
must be added to LD_LIBRARY_PATH.  After changing Rules.nvml, you must remake
linux-nvml.c. If you have already built PAPI, in
papi/src/components/nvml 'touch linux-nvml.c' so it will be recompiled, then in
papi/src 'make' to rebuild PAPI with the new name.

Other download packages may be available at 
https://developer.nvidia.com/gpu-deployment-kit

Before running the NVML component, the configure script for the NVML component 
must be executed in order to generate the Makefile which contains the 
configuration settings. This script needs to be executed only once.

It will help to have the env variable CUDA_DIR defined; on Summit this is automatically
defined by 'module load cuda'.  

    % cd < papi_dir >/src/components/nvml
    % ./configure --with-nvml-libdir=<NVMLDIR> --with-nvml-incdir=<NVMLDIR> --with-cuda-dir=<CUDADIR>

For example, one configuration may look like this
    %./configure --with-nvml-libdir=/usr/lib64/nvidia --with-nvml-incdir=/usr/local/cuda/include --with-cuda-dir=/usr/local/cuda

A recent (02/01/2019) NVML configure on the ICL saturn system:
./configure --with-nvml-libdir=$CUDA_DIR/lib64/stubs --with-nvml-incdir=$CUDA_DIR/include --with-cuda-dir=$CUDA_DIR

A recent (06/10/2019) NVML configure on the Summit system ( with $CUDA_DIR=/sw/summit/cuda/10.1.105 set by 'module load cuda' )

./configure --with-nvml-libdir=$CUDA_DIR/lib64/stubs --with-nvml-incdir=$CUDA_DIR/include --with-cuda-dir=$CUDA_DIR

Note the xxx/stubs/libnvidia-ml.so allows for linking PAPI, but only contains dummy routines;
At runtime you need a path to libnvidia-ml.so with live routines in LD_LIBRARY_PATH. It is
done this way so you can build PAPI (or any other NVML thing) without having the run-node's
file system structure; you can compile and link on the head node. 

The NVML component is added to PAPI during the configuration of PAPI
by adding the '--with-components=nvml' command line option to
configure.    

   % ./configure --with-components="nvml"

At build-time the nVidia compiler, nvcc, needs to be in your path, as does the cuda run-time library (libcudart.so).

Please refer to http://developer.download.nvidia.com/assets/cuda/files/CUDADownloads/NVML/nvml.pdf 
for details about NVML library.



Note: Power Limiting using NVML (aka power capping) requires root.

PAPI has added support for power limiting using NVML (on supported
devices from the Kepler family or later).  The executable needs to
have root permissions to change the power limits on the device.

The power_management_limit can be written to set a limit (in
milliWatts) to the power consumption by DEVICE.  The value that can
be written needs to be between the
power_management_limit_constraint_min and
power_management_limit_constraint_max.

nvml:::DEVICE:power_management_limit
nvml:::DEVICE:power_management_limit_constraint_min
nvml:::DEVICE:power_management_limit_constraint_max

A test for writing of the power_management_limit can be found in the
nvml/tests/ directory.
